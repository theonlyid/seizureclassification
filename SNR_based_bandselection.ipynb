{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Classifying seizures using spectral contrasting\n",
    " \n",
    " ## Testing the SNR based feature extraction technique\n",
    "\n",
    " Ali Zaidi\n",
    " \n",
    " Max Planck Institute for Biological Cybernetics, Tuebingen\n",
    " \n",
    " (c) All Rights Reserved\n",
    " \n",
    " ### BRIEF INTRODUCTION\n",
    "\n",
    " Our objective is to train a classifier to detect seizures from EEG data.\n",
    " We are using a data from the Temple University dataset.\n",
    " The objective of this analysis is to demonstrate the usefulness of smart\n",
    " feature engineering. The methods have already been defined in src/data_handling.py.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TABLE OF CONTENTS\n",
    "\n",
    "### [1. Understanding the data](#link1)\n",
    "An overview of EEG data used in this example\n",
    "### [2. The brut-force method](#link2)\n",
    "Demonstration of how throwing all the data into a classifier doesn't work. Also, PCA might reduce dimensions but still does not lead to any improvement.\n",
    "\n",
    "### [3. Spectral Contrasting](#link3)\n",
    "A demonstration of the intiution and implementation of spectral contrasting for timeseries analysis of complex signals.\n",
    "\n",
    "### [4. Classifying epochs](#link4)\n",
    "In the end, we would like to have a classifier that can return the label of an entire epoch (10s of EEG activity). This is a difficult problem because the dataset could have very limited (pre-)ictal (seizure) activity, and the rest o the 10s could be \"background\" activity, . This contaminates the labels and makes the problem very difficult.\n",
    "\n",
    "### [5. Taking it to the next level](#link5)\n",
    "The true test of a classifier, whether it can generalize to data it has never encountered before. The section performs 5x5 cross validation of the algorithm, and seperates data on the epoch level. The feature normalization is only performed on a subset of the data (the training set) and is tested on data with different internal dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all dependencies\n",
    "\n",
    "from __future__ import print_function, absolute_import\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import multiprocessing\n",
    "import scipy.stats\n",
    "from sklearn.manifold import TSNE\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data handling scripts.\n",
    "from src.data_handling import data_handling\n",
    "\n",
    "# Create an object from the class\n",
    "dh = data_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method load_data() imports the EEG data from 14 subjects and converts it to a d-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "A summary of valid labels is below: \n",
      "Format: [Label name, label index, Label count]\n",
      "['null', 0, 879]\n",
      "['bckg', 6, 6864]\n",
      "['gnsz', 9, 67]\n",
      "['cpsz', 11, 184]\n",
      "['tcsz', 15, 6]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dh.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking it to the next level: training and testing on separate sets of epochs  <a class=\"anchor\" id=\"link5\"> </a>\n",
    "\n",
    "Instead of using the entire dataset to obtain the normalization vector, we will train on one set of epochs and test on a completely new set of epochs. If we obtain high 5x5 cv accuracy, it means spectral normalization is doing a pretty good job of overcoming the non-stationarity in our feature-vector and is able to generalize across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.moveaxis(dh.data, -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "          decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "          max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now will going to perform 5x5 cross validation with random sampling and an 80/20 train-test ratio. This will demonstrate the power of the feature engineering paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = np.empty((dh.labels.shape[0], 1))\n",
    "\n",
    "idx_null = [idx for idx in range(dh.labels.shape[0]) if dh.labels[idx, 0] > 0]\n",
    "idx_bckg = [idx for idx in range(dh.labels.shape[0]) if dh.labels[idx, 6] > 0]\n",
    "idx_gnsz = [idx for idx in range(dh.labels.shape[0]) if dh.labels[idx, 9] > 0]\n",
    "idx_cpsz = [idx for idx in range(dh.labels.shape[0]) if dh.labels[idx, 11] > 0]\n",
    "idx_tcsz = [idx for idx in range(dh.labels.shape[0]) if dh.labels[idx, 15] > 0]\n",
    "\n",
    "y_labels[idx_null] = 0\n",
    "y_labels[idx_bckg] = 0\n",
    "y_labels[idx_gnsz] = 1\n",
    "y_labels[idx_cpsz] = 2\n",
    "y_labels[idx_tcsz] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM on training data subset\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "n_cv = 5\n",
    "\n",
    "f1score = np.empty((n_cv,))\n",
    "\n",
    "scores = np.empty((n_cv,))\n",
    "cc = np.empty((4,4,n_cv))\n",
    "\n",
    "for cv in range(n_cv):\n",
    "    print(\"Beginning run {} out of {}\".format(cv+1, n_cv))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, y_labels, stratify=y_labels, test_size=0.2, random_state=42*cv)\n",
    "\n",
    "    X_train = np.moveaxis(X_train, 0, -1 )\n",
    "\n",
    "    norm = dh.get_norm_array(X_train)\n",
    "\n",
    "    X_train_stft, _ = dh.get_stft(X_train, norm_array=norm)\n",
    "\n",
    "    idx_0 = [idx for idx in range(len(y_train)) if y_train[idx]==0]\n",
    "    idx_1 = [idx for idx in range(len(y_train)) if y_train[idx]==1]\n",
    "    idx_2 = [idx for idx in range(len(y_train)) if y_train[idx]==2]\n",
    "    idx_3 = [idx for idx in range(len(y_train)) if y_train[idx]==3]\n",
    "\n",
    "    X_0, y_0 = dh.generate_features_from_snr(X_train_stft[:,:,:,idx_0], 0)\n",
    "    # For a quick check on performance, comment code above and uncomment code below\n",
    "#     X_0, y_0 = dh.generate_features_from_snr(X_train_stft[:,:,:,idx_0[:len(idx_1)+len(idx_2)+len(idx_3)]], 0)  \n",
    " \n",
    "    X_1, y_1 = dh.generate_features_from_snr(X_train_stft[:,:,:,idx_1], 1)\n",
    "    X_2, y_2 = dh.generate_features_from_snr(X_train_stft[:,:,:,idx_2], 2)\n",
    "    X_3, y_3 = dh.generate_features_from_snr(X_train_stft[:,:,:,idx_3], 3)\n",
    "\n",
    "    X = np.append(X_0, X_1, axis=0)\n",
    "    X = np.append(X, X_2, axis=0)\n",
    "    X = np.append(X, X_3, axis=0)\n",
    "\n",
    "    y = np.append(y_0, y_1, axis=0)\n",
    "    y = np.append(y, y_2, axis=0)\n",
    "    y = np.append(y, y_3, axis=0)\n",
    "\n",
    "    ds = np.empty((X.shape[0], X.shape[1]+1));\n",
    "    ds[:,:-1] = X\n",
    "    ds[:,-1] = y\n",
    "\n",
    "    np.random.shuffle(ds)\n",
    "    \n",
    "    print(\"Training SVM on training data subset\")\n",
    "    clf.fit(ds[:,:-1], ds[:,-1])\n",
    "\n",
    "    def predict_epoch(epoch, clf, norm):\n",
    "        epoch_stft_norm, _ = dh.get_stft(epoch[:,:,np.newaxis], norm)\n",
    "        x, _ = dh.generate_features_from_snr(epoch_stft_norm, 0)\n",
    "\n",
    "        y_pred = clf.predict(x)\n",
    "\n",
    "        return np.median(y_pred).astype(int)\n",
    "\n",
    "    X_test = np.moveaxis(X_test, 0, -1)\n",
    "    \n",
    "    print(\"Testing SVM on test data subset\")\n",
    "    y_pred = np.empty((len(y_test),1))\n",
    "    for i in range(len(y_test)):\n",
    "        y_pred[i] = predict_epoch(X_test[:,:,i], clf, norm)\n",
    "\n",
    "\n",
    "    scores[cv] = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(\"Score for run {} is {}\".format(cv+1, scores[cv]))\n",
    "\n",
    "    cc[:,:,cv] = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    f1score[cv] = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"mean scores are %0.2f +/- %0.3f\" %(f1score.mean(), f1score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous accuracy was 0.95 +/- 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = np.sum(cc, axis=-1)\n",
    "CV1 = CV/np.sum(CV, axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, normalize=True):\n",
    "\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    classes=list(np.arange(4))\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           title='Normalized confusion matrix',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(CV1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy of cross-validation\n",
    "\n",
    "We got 95% accuracy by using a simple spectral-normalization technique, without any hyper-parameter tuning or optimisation.\n",
    "\n",
    "The accuracy is better than <a href=\"https://arxiv.org/pdf/1902.01012.pdf\">IBM Research</a>, however we've used a random subset of data.\n",
    "\n",
    "Further optimisation is currently underway, and the pipeline will then be tested on the full Temple-University dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balanced accuracy score is %0.2f +/- %0.3f\" % (scores.mean(), scores.std()))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
